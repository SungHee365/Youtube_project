from googleapiclient.discovery import build
import json
import re
import os

# YouTube API í‚¤ ê°€ì ¸ì˜¤ê¸°
API_KEY = os.getenv("YOUTUBE_API_KEY")
# YouTube API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
youtube = build("youtube", "v3", developerKey=API_KEY)

# ISO 8601 í˜•ì‹ì˜ ì‹œê°„ì„ "ì‹œê°„:ë¶„:ì´ˆ"ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def convert_duration(iso_duration):
    pattern = re.compile(r'PT(\d+H)?(\d+M)?(\d+S)?')
    matches = pattern.match(iso_duration)

    hours = int(matches.group(1)[:-1]) if matches.group(1) else 0
    minutes = int(matches.group(2)[:-1]) if matches.group(2) else 0
    seconds = int(matches.group(3)[:-1]) if matches.group(3) else 0

    return f"{hours}:{minutes:02}:{seconds:02}" if hours > 0 else f"{minutes}:{seconds:02}"

# ğŸ¯ ë² ìŠ¤íŠ¸ ëŒ“ê¸€ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_best_comment(video_id):
    try:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=1,  # ê°€ì¥ ì¢‹ì•„ìš”ê°€ ë§ì€ ëŒ“ê¸€ 1ê°œ ê°€ì ¸ì˜¤ê¸°
            order="relevance"
        )
        response = request.execute()

        best_comment = response["items"][0]["snippet"]["topLevelComment"]["snippet"]

        return {
            "text": best_comment["textDisplay"],
            "author": best_comment["authorDisplayName"],
            "like_count": best_comment["likeCount"]
        }

    except Exception as e:
        print(f"âŒ ëŒ“ê¸€ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None  # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜


#ì¹´í…Œê³ ë¦¬ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_video_categories(region_code="KR"):
    request = youtube.videoCategories().list(
        part="snippet",
        regionCode=region_code
    )
    response = request.execute()

    category_map = {item["id"]: item["snippet"]["title"] for item in response.get("items", [])}
    return category_map

#  ì˜ìƒ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ë² ìŠ¤íŠ¸ ëŒ“ê¸€ í¬í•¨)
def get_trending_videos(region_code="KR", max_results=10):
    category_map = get_video_categories(region_code)
    request = youtube.videos().list(
        part="id,snippet,contentDetails,statistics",
        chart="mostPopular",
        regionCode=region_code,
        maxResults=max_results
    )
    response = request.execute()

    videos = []
    for item in response.get("items", []):
        video_id = item["id"]
        best_comment = get_best_comment(video_id)  # ë² ìŠ¤íŠ¸ ëŒ“ê¸€ ì¶”ê°€
        category_id = item["snippet"].get("categoryId", "0")
        category_name = category_map.get(category_id, "ì•Œ ìˆ˜ ì—†ìŒ")  #ì¹´í…Œê³ ë¦¬ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°

        video_data = {
            "video_id": video_id,
            "title": item["snippet"]["title"],
            "channel_name": item["snippet"]["channelTitle"],
            "category": category_name,
            "duration": convert_duration(item["contentDetails"]["duration"]),
            "view_count": item["statistics"].get("viewCount", "0"),
            "thumbnail_url": item["snippet"]["thumbnails"]["high"]["url"],
            "upload_time": item["snippet"]["publishedAt"],
            "best_comment": best_comment
        }
        videos.append(video_data)

    return videos

#  JSON íŒŒì¼ ì €ì¥
SAVE_DIR = "data"
os.makedirs(SAVE_DIR, exist_ok=True)
SAVE_PATH = os.path.join(SAVE_DIR, "trending_videos.json")

trending_videos = get_trending_videos()

with open(SAVE_PATH, "w", encoding="utf-8") as file:
    json.dump(trending_videos, file, ensure_ascii=False, indent=4)

print(f" ë°ì´í„° ì €ì¥ ì™„ë£Œ: {SAVE_PATH}")
